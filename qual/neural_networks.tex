%\input{figures/net}
\section{Artificial Neural Networks}\label{sec:neural-networks}
\localtableofcontents
Artificial Neural Network algorithms for SR are all based on a Convolutional Neural Network (CNN) architecture.
%
We first briefly review ANNs in general and CNNs in particular and then explore the recent advances in Deep Learning (DL) for SR.
%
\subsection{Basics}
\input{figures/neural_networks/mlp.tex}
An Artificial Neural Network (ANN or NN) is a function specified by compositions of elementary functions called \textit{artificial neurons}\anote{ann} (or simply neurons).
%
Neurons consist of a set of inputs \(\mathbf{x} = (x_1, x_2, \dots, x_n)\), an aggregation (typically linear combination), and an activation function \(\sigma\), which acts as a thresholding mechanism.
%
For example, the simplest function that qualifies as a neuron is a linear function:
\begin{equation}
    z(\mathbf{x}) = w_1 x_1 + w_2 x_2 = \sum_i w_i x_i
    \label{eqn:simpleann}
\end{equation}
where the the activation function is the trivial one i.e., the identity.
%
Minsky \etal\cite{minsky2017perceptrons} famously proved that neurons that don't include a non-linear activation function have very little representational power (i.e., they're unable to represent functions as simple as even \(\operatorname{XOR}\)\anote{xor}.) and those that do are universal representers (i.e., given enough layers and neurons they're able to represent functions of arbitrary complexity).
%
Common non-trivial, i.e., non-linear, activation functions are the sigmoid function
\begin{equation}
    S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}
\end{equation}
or the hyperbolic tangent function
\begin{equation}
    \tanh x={\frac {\sinh x}{\cosh x}}={\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}={\frac {e^{2x}-1}{e^{2x}+1}}
\end{equation}
or the piece-wise defined \textit{rectified linear unit} (\(\operatorname{ReLU}\))
\begin{align}
    \operatorname{ReLU}(x) & \coloneqq \begin{cases}x&{\text{if }}x>0,\\0&{\text{otherwise}}\end{cases} \\
                           & = \max(0, x)
\end{align}
%
Note that eqn.~\eqref{eqn:simpleann} passes through the origin \((0,0,0)\) since it has no constant term; in the parlance of machine learning the neuron is missing a bias term\anote{bias} \(b\):
\begin{equation}
    z(\mathbf{x}) = \sum_i w_i x_i + b
    \label{eqn:linearregr}
\end{equation}
%
Neurons can be represented as directed graphs where a vertex represent an input or a neuron and edges represent the weights in the linear combination (see figure~\ref{fig:singleann}).
%
ANNs are then assemblies of neurons grouped into \textit{layers} with the layers composed by applying neurons to outputs from immediately preceding layers.
%
Those layers that are not input or output layers are denoted \textit{hidden} layers.
%
For example the ANN specified in figure~\ref{fig:multiann} represents the function
\begin{equation}
    \begin{split}
        y(\mathbf{x}) &= \sigma' \left( \sum_j w'_j z_j(\mathbf{x}) + b' \right) \\
        &=  \sigma' \left( \sum_{j=1}^m w'_j \sigma\left(\sum_{i=1}^n w_i x_i + b_j\right) + b' \right)
    \end{split}
\end{equation}

If ANNs were simply another way to diagrammatically represent non-linear functions they would be fairly uninteresting.
%
In fact ANNs are comprised by their definition and a \textit{learning} method.
%
The learning method enables the function to approximate some other function, by adjusting the weights \(w_i\), given \textit{training} pairs \(\left\{ \mathbf{x}_k, t_k \right\}\) where \(\mathbf{x}_k\) is the \(k\)th training \textit{sample} and \(t_k\) is the \(k\)th training \textit{target}.
%
The most common such learning rule is called the Delta rule\cite{widrow1960adaptive} for a single neuron, which can be derived from minimizing the the squared error \textit{loss} with respect to each of the weights for a given training pair \((\mathbf{x}_k, t_k)\):
\begin{equation}
    L(w_1, \dots, w_n) = \sum_k \frac{1}{2} (t_k - y(\mathbf{x}_k))^2
\end{equation}
and hence
\begin{equation}
    \pd{L}{w_i} = - \sum_k(t_k-y(\mathbf{x}_k))\cdot y'\cdot x_{ik}
\end{equation}
where here by \(y'\) we mean the derivative of the activation function with respect to its argument and by \(x_{ik}\) we mean the \(i\)th input \(x_i\) of the \(k\)th training sample.
%
Hence, by gradient descent the weights \(w_i\) should be adjusted in the opposite direction of \(\pd{L}{w_i}\) and so we have the weight adjustment rule
\begin{equation}
    \Delta w_i = \alpha \cdot \sum_k(t_k-y(\mathbf{x}_k))\cdot \sigma'\cdot x_{ik}
    \label{eqn:batchupdate}
\end{equation}
where \(\alpha\) is a small constant called the \textit{learning rate}.

The Delta rule is essentially the chain rule as applied to ANNs. 
%
In general computing the partial derivatives \(\pd{L}{w_i}\) for a deep (many layers) and wide (many neurons in each layer) network is onerous.
%
\input{figures/neural_networks/backprop.tex}
To mititage the effect of this combinatorial explosion of dependencies between the weights Rumelhart \etal\cite{rumelhart1988learning} popularlized a technique called \textit{back-propagation}\anote{backprop} or simply backprop (see figure~\ref{fig:backprop}).
%
Another inefficiency of the Delta rule is that it requires evaluating the ANN on the entire batch of samples in order to compute the adjustment \(\Delta w_i\).
%
For large training sets (on the order of millions of samples) this is infeasible due to memory limitations.
%
Stochastic Gradient Descent (SGD) replaces computing the \textit{batch loss} \(\sum_k(t_k-y(\mathbf{x}_k))\) in eqn.~\eqref{eqn:batchupdate} with an iterative update to \(w_i\):
\begin{equation}
    w_i^k = w_i^{k-1} + \alpha \cdot (t_k-y(\mathbf{x}_k))\cdot \sigma'\cdot x_{ik}
    \label{eqn:sgd}
\end{equation}
Equation~\eqref{eqn:sgd} is evaluated for each of the \(k\) samples sequentially and therefore saves having to store all training samples in memory.
\subsection{Convolutional Neural Networks}

\subsection{Deep Neural Networks}
\input{figures/neural_networks/deep_archs}