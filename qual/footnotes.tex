%! Suppress = GatherEquations
% introduction
\anotecontent{vbsuper}{We will often use the verb form ``to super resolve'' in order to denote the use of one or more such methods.}
\anotecontent{rayleighscriterion}{The amplitude of the diffraction pattern (known as the Airy pattern) of a monochromatic point source through a circular aperture is given by \[I(\theta)\coloneqq I_{0}\left[{\frac {2J_{1}(ka\sin \theta )}{ka\sin \theta }}\right]^{2}\] where \(I_0\) is peak intensity (at the center), \(k=\frac{2\pi}{\lambda}\) is the wavenumber of the light, \(\theta\) is the angle of observation, and \(J_1\) is the Bessel function of the first kind of order one \cite{goodman2005introduction}.
	%
	It is maxima/minima of this function that Rayleigh's criterion concerns.}
\anotecontent{shotnoise}{The number of photons incident on a sensor element is distributed according to a Poisson distribution (arrivals are independent and their rate is constant). For a small number of photons the variance in total arrivals (i.e., image brightness) is high.}
\anotecontent{satelliteoptics}{Rayleigh's criterion implies that the angular resolution \(R\) of a telescope with optical diameter \(D = 2.4\text{m}\) observing visible light (\({\sim}500\text{nm}\)) is approximately \cite{doi:10.1080.14786447908639684} \[R  \approx 1.220\frac{\lambda}{D} = 1.220 \frac{500\text{nm}}{2.4\text{m}} \approx 0.06\text{arcsec}\] From an altitude of 250 km this corresponds to a ground sample distance of 6cm.
	%
	This loss of resolving power is further exacerbated by refraction through turbulent atmosphere \cite{Fried:66}.}
\anotecontent{subpixel}{For example when a point source wholly captured by one sensor element shifts to distributing energy equally amongst that same element and a direct adjacent.}

% background
\anotecontent{lexico}{Consider an \(N\) row \(\times\) \(M\) column image \(X(n, m)\).
	%
	The column-major vectorization \(bm{x}\) of the image is the \(NM\) length vector defined as \(\bm{x}(Mm + n) = X(n, m)\).}
\anotecontent{patch}{\(m \times m\) pixel window, e.g. \(3 \times 3\).}
\anotecontent{tensor}{A multidimensional array. Not to be confused with the algebraic object.}
%\anotecontent{deep-depletion}{The depletion layer is the region at the Si-SiO\(_2\) interface that is characterized by few charge carriers (electrons and holes) that occurs when a gate voltage higher than the flat-band voltage is applied. Deep-depletion is characterized by a depletion region width that is greater than that at equilibrium \cite{semiconductorbook}.}
%\anotecontent{inversion}{The term inversion means that the surface is inverted from P type to N type, or electron rich. This process is a slow thermal equilibration \cite{ccdarkcurrent}, is called dark current and at various temperatures/exposure times is a source of noise.}
\anotecontent{nyquist}{Twice the frequency of the lowest frequency component in the image. Sampling at a rate below this leads to aliasing of high frequency components as low frequency components.}
\anotecontent{depletion}{By applying a positive voltage to the gate holes are repelled and migrate through the body to ground.
	%
	This leaves a depleted region near the Si-SiO\(_2\) interface that acts as an insulator.}
\anotecontent{buriedchannel}{Buried channel as distinct from surface channel.
	%
	In surface channel the signal charge is stored at the Si-SiO\(_2\) interface, which can lead to charge trapping during the charge transfer process \cite{Bass:2009:HOT:1594759}.}
\anotecontent{ntype}{Semiconductors are classified as either \textit{n-type} or \textit{p-type}, the former being doped with negative charge carriers and the latter being doped with positive charge carriers (holes).}
\anotecontent{ccd}{CCD arrays, for example, employ \(2\times 2\) or \(3\times 3\) pixel binning, which is the practice of collapsing windows of pixels down to one pixel.}
\anotecontent{pnjunction}{The interface between a p-type semiconductor and a n-type semiconductor.}
\anotecontent{reversebias}{With the p-type material at a lower voltage than the n-type.
	%
	This causes both the holes and the electrons to flow away from the junction creating a depletion zone.}
\anotecontent{bolometer}{Late 19th century: from Greek bolÄ“ `ray of light' + -meter.}
\anotecontent{mems}{Micro-electro-mechanical systems.}
\anotecontent{fabryperot}{An optical cavity made from two parallel reflecting surfaces that passes light only when it is in resonance with the cavity.}
\anotecontent{thermistor}{An element with an electrical resistance that is a function of its temperature.}
\anotecontent{innerphotoelectric}{The photoelectric effect is the emission of electrons when light hits a material.
	%
	The inner photoelectric effect is that phenomenon in the bulk matter of semiconductors.}
\anotecontent{otf}{The optical transfer function is the spatial Fourier transform of the point spread function (the impulse response) of the optics.
	%
	Spatial here means periodic in space rather than in time.}
\anotecontent{coma}{Off-axis point sources appearing to have a tail (coma), due to variation in magnification in the image of the aperture stop.}
\anotecontent{dispersion}{A well known example of this phenomenon is the cover of Pink Floyd's classic album The Dark Side of the Moon.}
\anotecontent{fnumber}{The ratio of the system's focal length to the diameter of the aperture.}
\anotecontent{registration}{The process of aligning two or more images of the scene on a common pixel grid.}
%registration
\anotecontent{scalepyramid}{No pun intended.}
\anotecontent{threshold}{Set everything below a threshold to zero.}
\anotecontent{nms}{Pick the maximum in a neighborhood and set all other values to zero.}
\anotecontent{log}{The Laplacian of an image \(X\) is \(L(x,y) = \partial_x^2 X + \partial_y^2 X\).
	%
	Since in practice this approximates a noisy signal (second derivative), smoothing by a Gaussian is a necessary prerequisite.
	%
	Therefore the Laplacian of Gaussians (\(LoG\)) filter is the Laplacian composed with the Gaussian: \begin{equation*} LoG(x,y, \sigma) \coloneqq -\frac{1}{\pi \sigma^4}\left[ 1 - \frac{x^2+y^2}{2\sigma^2} \exp \left\{ - \frac{x^2+y^2}{2\sigma^2} \right\} \right]
	\end{equation*}}
\anotecontent{smallcurvature}{The ratio of the eigenvalues of the spatial Hessian of \(D\) (i.e., only along dimensions \(x,y\)).
	%
	In fact a quantity not unlike eqn.~\eqref{eqn:strength} is computed in order to save having to explicitly find eigenvalues.}
\anotecontent{hog}{The histogram of gradient orientations (polar angle) in a pixel window.}
\anotecontent{kdtree}{\(k\)-d trees \cite{Bentley:1975:MBS:361002.361007}, short for \(k\)-dimensional trees, are data structures that partition space efficiently in order that searching the data structure, insertion into the data structure, and deletion from the data structure are all, on average, \(O(\log n)\) time operations (where \(n\) is the number of nodes in the tree at the time of the operation).}
\anotecontent{ransac}{Random Sample Consensus (RANSAC) is a method used to estimate parameters of a model given outliers.
	%
	In this case Zahra \etal use RANSAC at the transform estimation step to eliminate falsely matched keypoint pairs.
	%
	RANSAC iterates by repeatedly random sampling the putative matching keypoint pairs and fitting a transform model.
	%
	At a given iteration the fitted transform model is tested against the heldout keypoint pairs and evaluated (according to goodness of fit on a subset called the \textit{consensus set}).}
\anotecontent{gridsearch}{Searching a space by taking regular incremental steps along each dimension. For example, search \(U \subset \mathbb{R}^2\) by checking all \(x = \Delta x + x_0, y = \Delta y + y_0\) for \((x_0, y_0) \in U\) and for fixed \(\Delta x, \Delta y\).}
\anotecontent{fouriershift}{If \(\mathcal{F}\{\left[ x_n \right] \}_{k}=X_{k}\) is the \(k\)-th frequency component of the discrete signal
\(\left[ x_n \right]\) then shifting \(\left[ x_n \right]\) by \(m\) steps implies
\[
	\mathcal{F}\left\{\left[ x_{n-m} \right]\right\}_{k} = X_{k}\cdot e^{-{\frac {i2\pi }{N}}km}
\]
}
\anotecontent{hadamard}{
	\[
		\begin{bmatrix}
			a_{11} & a_{12} \\
			a_{21} & a_{22} \\
		\end{bmatrix}\odot \begin{bmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} \\
		\end{bmatrix} \coloneqq
		\begin{bmatrix}
			a_{11}\,b_{11} & a_{12}\,b_{12} \\
			a_{21}\,b_{21} & a_{22}\,b_{22} \\
		\end{bmatrix}
	\]
}
\anotecontent{shannon}{
	Shannon entropy can be interpreted as the average number of bits \(L\) necessary to encode a random variable \(X\) distributed according to distribution \(P\).
	%
	It is defined as
	\[
		H(X)\coloneqq\sum{P(x)\log\left[\frac{1}{P (x)}\right]}
	\]
	i.e., \(P(x_i) = 1/2^L\) where \(L\) is the number of bits.
}
\anotecontent{kde}{Alternatively known as Parzen windowing, the kernel density estimator \(\hat{P}\) of a distribution \(X\sim P\) given a set of points \(\left\{ x_1, x_2, \dots, x_n \right\}\) is defined as
	\[
		\hat{P}(x) \coloneqq {\frac {1}{nh}}\sum _{i=1}^{n}K\left( \frac {x-x_{i}}{h} \right)
	\]
	where \(K\) is a kernel (e.g. Gaussian) and \(h\) is the \textit{kernel bandwidth}.
}
\anotecontent{convexhull}{A convex combination of points \(x_1, x_2, \dots, x_n\) is all \(\alpha _{1}x_{1}+\alpha _{2}x_{2}+\cdots +\alpha _{n}x_{n}\) where \( \alpha _{i}\geq 0\) and \(\alpha _{1}+\alpha _{2}+\cdots +\alpha _{n}=1\).
%
The convex hull of a set of points \(X\) is the set of all convex combinations of points in \(X\).
%
Figuratively one can imagine stretching a rubber band such that it contains all points in \(X\).}
\anotecontent{completegraph}{A graph in which each pair of vertices is connected by an edge.}
\anotecontent{homogeneous}{
	With \((x,y)\) a pixel coordinate, transformations \(T\) in homogeneous coordinates operate on \((x,y,1)\): if
	\(
	(a,b,c) = (x,y,1)\cdot T
	\)
	then the transformed pixel coordinate
	\(
	(x',y') = \frac{1}{c} (a,b)
	\).
}
\anotecontent{advection}{Advection is the transport of a substance, or properties thereof, by bulk flow of a fluid. The substance, or properties thereof, are \textit{advected} by the flow and are considered \textit{conserved}. In general, the fluid flow is described by a vector field \(\mathbf{u}\) (called velocity) and the substance or property is described by a scalar field \(\psi\) (variously called distribution, concentration, or density). The canonical example is the advection of ink by flowing water.}
\anotecontent{stokes}{
	The integral of a differential form \(\omega\) on the boundary of some orientable manifold \(\Omega\)  is equal to the integral of its exterior derivative \(\dif \omega\) on \(\Omega\), i.e
	\[
		\oint\limits _{\partial \Omega }\omega =\int\limits _{\Omega }\dif\omega
	\]
	In this instance \(\Omega = R\), \(\omega = \mathbf{j} \cdot \mathbf{n}\, \dif l \) and \( \dif \omega = \nabla \cdot \mathbf{j}\, \dif R\).
}
\anotecontent{tvloss}{ Total variation is equivalent to the integral of the gradient magnitude:
\[
	\abs{u}_{\operatorname {TV}}=\int\limits\abs{\nabla u}
\]
}
\anotecontent{indicator}{The indicator (or characteristic) \(\mathbbm{1}_{[a,b)}\) function is 1 on the interval \([a,b)\) and 0 otherwise.}
\anotecontent{vectorspace}{A set whose members (\textit{vectors}) can be decomposed as linear combinations of elementary elements (a \textit{basis}).}
\anotecontent{splinecontconstr}{Note that even though a \(d\)-degree polynomial has \(d\) derivatives we only require agreement at the first \(d-1\) derivatives (and continuity itself). This is owing to the fact that if we required all \(d\) derivatives to agree the number of degrees of freedom would be \((n(d+1) - (n-1)(d+1) = d+1\) and therefore the spline would no longer be a piecewise polynomial but simply a polynomial (of degree \(d\)).}



%classical
\anotecontent{delauney}{The Delaunay triangulation for a set of points \(P\) is a triangulation such that no point in \(P\) is inside the circumcircle of any triangle in the triangulation. Equivalently it is the triangulation that maximizes the minimum angle of all of the triangles in the triangulation.}
\anotecontent{estiminterp}{This blurring of the distinction between interpolation and estimation is to be expected since estimation is a type of regression.}
\anotecontent{gp}{A Gaussian process is a sequence of random variables such that any finite subset is distributed multivariate Normal.}
\anotecontent{lsi}{In analogy with Linear Time Invariant (i.e., linear and constant in space).}
\anotecontent{lrpixel}{An LR pixel is one sampled from an LR image and embedded in the HR grid.
	%
	An HR pixel is a pixel in the HR grid.}
\anotecontent{vectorize}{
	\begin{multline*}
		\text{vec}\left(
		\begin{bmatrix}
				a_{11} & a_{12} & \cdots & a_{1n} \\
				a_{21} & a_{22} & \cdots & a_{2n} \\
				\vdots & \vdots & \ddots & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn}
			\end{bmatrix}
		\right) = \\
		\left[a_{11}, \mathellipsis, a_{m1}, a_{12}, \mathellipsis, a_{m2}, \mathellipsis, a_{1n} \mathellipsis a_{mn}\right]
	\end{multline*}
}
\anotecontent{noise}{Noise here doesn't necessarily mean unwanted high frequency variation but simply a source of randomness.
	%
	This is in close affinity with how generative machines such as variational auto-encoders and generative adversarial networks are understood.}
%
\anotecontent{gpassumptions}{Namely \(\bm{y}_1\) is distributed \((0, W)\)-Normal and independent of \(\eta_1\).}
\anotecontent{kalmanprediction}{This is better understood in the more general linear dynamic systems case where \(\bm{y}_k = A_k\cdot\bm{y}_k + B_k \cdot \bm{u}_k + \varepsilon_k\) and \(B_k \cdot \bm{u}_k\) is known as controlled input.
	%
	Then the prediction includes a \(B_k \cdot \bm{u}_{k-1}\) term and the Kalman gain effectively mediates between controlled and uncontrolled inputs.
}
\anotecontent{hmrf}{A Markov Random Field (MRF) is a collection of random variables \(x_i, y_j, \mathellipsis\) with conditional dependence represented by pairings and satisfying the \textit{pairwise Markov} property: any two random variables that aren't paired are conditionally independent of each other given (conditioned on) all other variables.
	%
	A \textit{Hidden} Markov Random Field is simply a MRF where some of the random variables aren't observed.
}
\anotecontent{locallinear}{An LR patch \(x_i\) on the LR manifold and its neighbors \(x_j, x_k, \mathellipsis\) lie in a locally linear subspace of the manifold.}
\anotecontent{forest}{A tree is a graph in which any pair vertices is connected by exactly one or zero edges. A forest is a disjoint collection of trees.}
\anotecontent{positivedef}{A symmetric real matrix \(Q\) is positive definite if \(\bm{y}^T Q \bm{y} > 0 \) for all non-zero \(\bm{y}\).}
\anotecontent{conjugategradients}{Two vectors \(\bm{u}, \bm{v}\) are conjugate with respect to \(G\) if \(\bm{u}^T G \bm{v} = 0\).
	%
	Conjugate gradient descent is gradient descent but with conjugate gradients (it has better convergence properties).
}
\anotecontent{illcondition}{The condition number \(\kappa\) of a function is a measure of how sensitive it is to small perturbations; for a matrix \(G\) it is defined \(\kappa (G) \coloneqq \sigma_{\max}(G)/\sigma_{\min} (G)\) where \(\sigma_{\max}(G), \sigma_{\min} (G)\) are the maximum and minimum singular values of \(G\) respectively.}
\anotecontent{precondition}{A preconditioner of a matrix \(G\) is an approximation of \(G\) that has a better condition number.
	%
	Nguyen \textit{et al.} propose a preconditioner with singular values clustered around 1 in order that \(\kappa(G) \approx 1\).}
\anotecontent{gradientoperator}{Let \(u = (1,2,1)\) and \(v = (1,0,-1)\) then \(h = uv^T\) is the first order Sobel filter and \(\nabla Y = \left( h \ast Y, h^T \ast Y \right)\).}
\anotecontent{boundedvariation}{\(f(x)\) is of bounded variation on interval \(\left[ a,b \right]\) if over all partitions \(\mathcal{P}\) of \(\left[ a,b \right]\)
	\[
		\sup_{P \in \mathcal{P}}\sum_{i=0}^{n_P-1} \lvert f(x_{i+1})-f(x_i) \rvert\, <\infty
	\]
}
\anotecontent{beliefpropagation}{An efficient way to compute marginals of joint probabilities that have structure by reusing partial sums (i.e., passing messages) \cite{Yedidia2003jan1}.}
\anotecontent{manifold}{A collection of points that locally resembles Euclidean space.}
\anotecontent{manifoldlearning}{Dimensionality reduction.}
\anotecontent{pun}{No pun intended.}
\anotecontent{overcompletedictionary}{A collection of vectors (known as \textit{atoms}) that spans a space but is also linearly dependent.}
\anotecontent{nphard}{Problems in NP are those which are unknown to have a deterministic polynomial time solutions.
	%
	An NP-hard problem admits a polynomial time reduction from any problem in NP (colloquially they are as hard as any problem in NP).}
\anotecontent{firstordergaussian}{The first derivative of the Gaussian expressed as a filter: \(\left[ -2,-1,0,1,2 \right]\).}

% ann
\anotecontent{ann}{Artificial neurons are loosely inspired by the spiking neuron model of biological neurons, where dendrites correspond to inputs, the soma corresponds to a linear sum of inputs, and axons corresponds to the activation function.}
\anotecontent{bias}{The semantics of bias are inherited from from statistics, where the bias of an \textit{estimator} is the extent to which the estimator, on average, differs from the value being estimated. Therefore, if we regard an artificial neuron as an estimator (a nonlinear regression), we see that the constant term \(b\) indeed determines its bias.
}
\anotecontent{xor}{Exclusive-Or \(\operatorname{XOR}(a,b)\) is a Boolean function (maps \(\{0,1\}\) to \(\{0,1\}\)) defined to be 1 when \(a \neq b\) and 0 otherwise.}
\anotecontent{backprop}{Backprop itself is a particular instance of a general technique called automatic differentiation (AD). AD computes exact derivatives of functions in either \textit{forward accumulation} or \textit{reverse accumulation} \cite{linnainmaa1976taylor}. Backprop corresponds to reverse mode. In reverse mode the dependent variable \(y\) to be differentiated is fixed and the derivative is computed with respect to each sub-expression \(z_i\) recursively:
	\begin{multline*}
		{\frac  {\partial y}{\partial w}}={\frac  {\partial y}{\partial z_{1}}}{\frac  {\partial z_{1}}{\partial w}}=\left({\frac  {\partial y}{\partial z_{2}}}{\frac  {\partial z_{2}}{\partial z_{1}}}\right){\frac  {\partial z_{1}}{\partial w}}=\\\left(\left({\frac  {\partial y}{\partial z_{3}}}{\frac  {\partial z_{3}}{\partial z_{2}}}\right){\frac  {\partial z_{2}}{\partial z_{1}}}\right){\frac  {\partial z_{1}}{\partial w}}=\cdots 
	\end{multline*}
}
\anotecontent{universapprox}{Given enough layers and neurons ANNs with nonlinear activations are able to represent functions of arbitrary complexity. Conversely, sans a nonlinear activation function, ANNs are unable to represent functions as simple as even \(\operatorname{XOR}\).}
\anotecontent{learn}{To learn, in this context, means to approximate a function that effectively performs a task. For example in the case of an object recognition task, learning entails approximating a function that outputs high values when an object is recognized and low values otherwise.}
\anotecontent{largeparams}{OpenAIâ€™s Generative Pre-trained Transformer (GPT) \cite{radford2018improving} natural language processing (NLP) network has approximately 150 million parameters (and takes 8 GPU-months of non-stop training).}
\anotecontent{kldiv}{The Kullbackâ€“Leibler divergence of two probability distributions \(P,Q\) with densities \(p, q\) is defined
	\begin{equation*}
		\operatorname{D}_{\text{KL}}(P\parallel Q)\coloneqq\int\limits _{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx
	\end{equation*}
Note that \(\operatorname{D}_{\text{KL}}\) is not symmetric and therefore not a true distance measure. Fortunately it can be symmetrized:
\begin{equation*}
	\operatorname{JSD}(P\parallel Q)\coloneqq{\frac  {1}{2}}\operatorname{D}_{\text{KL}}\left(P\bigg\| {\frac  {1}{2}}(P+Q)\right)+{\frac  {1}{2}}\operatorname{D}_{\text{KL}}\left(Q\bigg\| {\frac  {1}{2}}(P+Q)\right)
\end{equation*}
In this form it is called the Jensenâ€“Shannon divergence and is a true distance measure between probability distributions.
}
\anotecontent{minimax}{Strictly speaking mini-max is a decision rule followed by each player of a game --- a decision rule that leads to minimizing the possible loss for a worst case (maximum loss) scenario.}
\anotecontent{uniformconv}{A sequence of functions \(F_n\), \textit{uniformly converges} to \(f\) on a bounded interval \(I\) if given some \(\varepsilon > 0\) there exists \(n\) such that 
\[
	\abs{ F_n( x ) - f ( x ) } < \varepsilon
\]
for all \(x \in I\). Note the order of the quantifiers: there should be \textbf{one} \(n\) that satisfies the constraint at a given precision \(\varepsilon\) for \textbf{all} \(x \in I\) simultaneously.
}
\anotecontent{annealing}{Learning rate annealing imposes a scheduled decay of the learning rate \(\alpha_i\) over time. The intuition being that initially, prior to identifying a minimum, one can afford to take optimistically large steps \(\Delta w_i\) in parameter space but over time one should become more conservative in order to hone in on an identified minimum. Typically the annealing schedule is such that the learning rate out starts out large and is reduced by an order of magnitude (i.e., a factor of 10) every \(M\) epochs (where \(M\) is a divisor of the total number of epochs that one will train for).}
\anotecontent{gradclip}{Exploding gradients is the opposite of vanishing gradients: gradients become so large that weights become unstable. That is, weight updates become large and oscillate around a minimum. Gradient clipping constrains \(\pd{L}{w_i}\) to be within a narrow range around 0; Kim \etal constrain the gradient to be within \(\left[ -\frac{\theta}{\alpha_i}, \frac{\theta}{\alpha_i} \right]\) for some small \(\theta\).}
\anotecontent{ntire}{The performances of various techniques on SISR and MISR tasks are evaluated at a competition held at the annual New Trends in Image Restoration and Enhancement (NTIRE) workshop. The competition \cite{nah2019ntire} has two tracks: a clean track that only assumes down-sampling test cases and a blur (or natural) track that assumes motion blur (in an effort to approximate real conditions). Many teams compete in both tracks.}

% conclusions
\anotecontent{transfer}{Given a source domain and target domain, transfer learning sets out to accelerate learning for a task over the target domain using learning already performed for a task over the source domain.}
\anotecontent{tremor}{For example Physiologic tremor in the case of cell phone cameras. Physiologic tremor is a low amplitude tremor of a limb that presents at approximately 10Hz. It occurs in normal, healthy individuals.}
\anotecontent{rl}{Reinforcement Learning is a paradigm of machine learning based on principles of operant conditioning, i.e., using punishment and reward.}
\anotecontent{rnn}{A Recurrent Neural Network (RNN) models time dependencies in sequences of data.}

% appendix
\anotecontent{diracdelta}{A generalized function is a function that can only be integrated against. Let 
\begin{equation*}
	\delta_\epsilon(x) \coloneqq \frac{1}{\epsilon \sqrt{\pi}}	e^{-x^2/\epsilon^2}
\end{equation*}
Then for any continuous \(\phi(x)\)
\begin{equation*}
	\lim_{\epsilon\rightarrow 0} \left[\int\limits_{-\infty}^{\infty} \phi(x)\delta_{\epsilon}(x-x_0) \right] = \lim_{\epsilon\rightarrow 0} \left[ \phi(x_0)\int\limits_{-\infty}^{\infty} \delta_{\epsilon}(x-x_0) \right] = \phi(x_0)
\end{equation*}
and hence \(\delta(x)\) is defined as the limit of this behavior
\begin{equation*}
	\int\limits_{-\infty}^{\infty} \phi(x)\delta(x-x_0) = \phi(x_0)
\end{equation*}
}